{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bk92nT2wT4U8"
      },
      "source": [
        "# Computer Vision Homework 02 - Compression with Autoencoders and Implicit Neural Representations\n",
        "\n",
        "Contact: David C. Schedl (david.schedl@fh-hagenberg.at)\n",
        "\n",
        "Note: this is the starter pack for the **Computer Vision** homework. You do not need to use this template! \n",
        "Which framework you use is up to you!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GMb4sAogUarc"
      },
      "source": [
        "For this exercise the task is to compress images (again). In comparison to HW01 do it with deep learning, now. \n",
        "You can should compare two approaches: Autoencoders and Implicit Neural Representations. Note, it is also possible to somehow combine both approaches.\n",
        "\n",
        "Analyze your algorithm on a set of images and evaluate your results concerning quality (e.g., MSE) and size reduction achieved (how many bits could be saved?). What are there available parameters and how can you tune them?\n",
        "\n",
        "Ideally you use set of different evaluation images (e.g., showing artificial content, buildings, nature, animals, objects, colors, â€¦). For the AutoEncoers make sure that the same images are not in your training set.\n",
        "\n",
        "*Hint:* Work with low-res images and small networks to speed up computation and avoid huge storage requirements.\n",
        "\n",
        "\n",
        "**Further comments/hints:**\n",
        "\n",
        "*   You do not need to come up with a super network architecture! It is mostly about getting into the topic.\n",
        "*   It is up to you if you want to use a CNN or a classical NN with MLPs (dense layers) for the Autoencoder. \n",
        "*   Think about the problem, solve it, and critically evaluate your solution.\n",
        "*   For this exercise, it makes sense to compare it to your results of HW01.\n",
        "*   Summarize your ideas and findings in the report. \n",
        "\n",
        "\n",
        "\n",
        "**Have fun!** ðŸ˜¸"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Approach 1: Autoencoders (e.g., using TensorFlow or PyTorch)\n",
        "\n",
        "An autoencoder is a neural network that learns efficient codings of unlabeled data (so it uses unsupervised learning). If the encoding is much smaller than the initial data you can think of the algorithm as compression.\n",
        "That's what we want to do in this task!\n",
        "\n",
        "Implement an autoencoder that compresses images. The dataset that you train it with and the resolution is up to you. \n",
        "Evaluate how small the encoding can get before the image quality gets pretty bad?\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VWPV4HPAoTm1"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "As always let's import useful libraries, first. \n",
        "This notebook works with TensorFlow and is designed to run in a Colab Environment. \n",
        "Getting everything running locally might take some time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unFhN9evoUQV"
      },
      "outputs": [],
      "source": [
        "%%capture \n",
        "# use %% capture suppress any output\n",
        "\n",
        "# make sure to use tensorflow 2.x\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "# load the TensorBoard notebook extension to display training results later\n",
        "%load_ext tensorboard\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow.keras.losses as losses\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.layers import (Dense, Flatten, Conv2D, MaxPooling2D, \n",
        "                                     Conv2DTranspose, Reshape, Input)\n",
        "\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "def MSE(A,B):\n",
        "  \"\"\"compute the mean squared error (MSE) between numpy array A and B\n",
        "  \"\"\"\n",
        "  return ((A - B)**2).mean(axis=None)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qNfPtXgOVPlk"
      },
      "source": [
        "## Example Dataset: CIFAR 10\n",
        "\n",
        "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. It was originally used for Machine Learning, but we can use it for our compression experiments.\n",
        "For details, see [this website](https://www.cs.toronto.edu/~kriz/cifar.html)!\n",
        "\n",
        "The dataset is readily available as TensorFlow/Keras dataset. \n",
        "In HW01 we used a different way to load the data (without TensorFlow).\n",
        "\n",
        "**Note: you do not need to use CIFAR-10! Any other dataset is fine.** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdI7V-ijTuup",
        "outputId": "575e8d6c-a07e-464d-8342-790e30e68199"
      },
      "outputs": [],
      "source": [
        "(x_train, _), (x_test, _) = datasets.cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "3rB2LBylWwsJ",
        "outputId": "66a93595-67a0-4737-d742-06e23afa003b"
      },
      "outputs": [],
      "source": [
        "# display one training and one test image\n",
        "\n",
        "train_image = x_train[0]\n",
        "test_image = x_test[0]\n",
        "\n",
        "plt.subplot(1,2,1), plt.imshow(train_image), plt.title('training image')\n",
        "plt.subplot(1,2,2), plt.imshow(test_image), plt.title('test image')\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yAZuFN-iIXxf"
      },
      "source": [
        "## Example AutoEncoder with Multi-Layer Perceptrons (Dense layers)\n",
        "\n",
        "An implementation of an MLP autoencoder which encodes images to a latent vector. \n",
        "For further details see this [Tensorflow Tutorial](https://www.tensorflow.org/tutorials/generative/autoencoder).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_d88Kp_2E5U",
        "outputId": "4ec38605-5471-48d3-a890-a14d2057f6c1"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(Model):\n",
        "  def __init__(self, latent_dim, image_shape):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.latent_dim = latent_dim   \n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      Flatten(),\n",
        "      Dense(latent_dim, activation='relu'),\n",
        "    ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      Dense(np.prod(image_shape), activation='sigmoid'),\n",
        "      Reshape(image_shape)\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "latent_dim = 64 \n",
        "image_shape = x_train[0].shape\n",
        "autoencoder = Autoencoder(latent_dim, image_shape)\n",
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "autoencoder.build(x_test.shape)\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG_4KTzg54k3"
      },
      "outputs": [],
      "source": [
        "# Training Preparation (if you want to use TensorBoard)\n",
        "\n",
        "# Place the logs in a timestamped subdirectory\n",
        "# This allows to easy select different training runs\n",
        "# In order not to overwrite some data, it is useful to have a name with a timestamp\n",
        "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Specify the callback object\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# tf.keras.callback.TensorBoard ensures that logs are created and stored\n",
        "# We need to pass callback object to the fit method\n",
        "# The way to do this is by passing the list of callback objects, which is in our case just one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJ6psn7F59Rq"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit\n",
        "# open the TensorBoard here!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tbNO_oSgJD3Z"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGB5T28C3QV7",
        "outputId": "59a4f402-c2f4-4723-9d19-8370d01a93a5"
      },
      "outputs": [],
      "source": [
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                callbacks=[tensorboard_callback],\n",
        "                verbose=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r5e8vIMKJGdk"
      },
      "source": [
        "### Encoding / Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA9dGsAf4w8e",
        "outputId": "2c6f3444-5a71-44ab-ac0a-872152f162dd"
      },
      "outputs": [],
      "source": [
        "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
        "print(encoded_imgs.shape) # every encoded image is represented by 64 values\n",
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "qHoaIcDu4zLz",
        "outputId": "bace6111-60d0-4826-deea-47d4221cabe4"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "  # display original\n",
        "  ax = plt.subplot(2, n, i + 1)\n",
        "  plt.imshow(x_test[i])\n",
        "  plt.title(\"original\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  mse = MSE(x_test[i], decoded_imgs[i])\n",
        "\n",
        "  # display reconstruction\n",
        "  ax = plt.subplot(2, n, i + 1 + n)\n",
        "  plt.imshow(decoded_imgs[i])\n",
        "  plt.title(f\"MSE:{mse:.2E}\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ONZUGuCxJceK"
      },
      "source": [
        "## Example AEs with Convolutional Layers\n",
        "\n",
        "Below you will find two variants of autoencoders that use convolutions to downsample and transposed convolutions to upsample. \n",
        "\n",
        "The second variant uses dense layers to further downsample. Note that dense layers result in a huge number of parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmuYJXQxHDVV",
        "outputId": "0f62ba5e-f756-49b4-b1ab-faa5469de3b2"
      },
      "outputs": [],
      "source": [
        "# a convolutional auto encoder with convolutional and transposed convolutional layers \n",
        "class ConvAutoencoderV1(Model):\n",
        "  def __init__(self, latent_dim, image_shape):\n",
        "    super(ConvAutoencoderV1, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      Input(shape=image_shape),\n",
        "      Conv2D(16, (3, 3), activation='relu', padding='same', strides=2), # downsample x2\n",
        "      Conv2D(8, (3, 3), activation='relu', padding='same', strides=2),  # downsample x2\n",
        "      Conv2D(4, (3, 3), activation='relu', padding='same', strides=2),  # downsample x2\n",
        "      # 4x4x4 = 64\n",
        "    ])\n",
        "\n",
        "    latent_img_shape = (8,int(image_shape[0]/4),int(image_shape[1]/4))\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      Conv2DTranspose(4, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      Conv2D(image_shape[2], kernel_size=(3, 3), activation='sigmoid', padding='same') # back to color\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "latent_dim = 64 \n",
        "image_shape = x_train[0].shape\n",
        "autoencoder = ConvAutoencoderV1(latent_dim, image_shape)\n",
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "autoencoder.build(x_test.shape)\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V8be9C5HKCW",
        "outputId": "eec0a429-ba76-41fd-f542-760294844e5d"
      },
      "outputs": [],
      "source": [
        "# a convolutional auto encoder with additional dense layers \n",
        "# Note that this has a lot more parameters!\n",
        "\n",
        "class ConvAutoencoderV2(Model):\n",
        "  def __init__(self, latent_dim, image_shape):\n",
        "    super(ConvAutoencoderV2, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      Input(shape=image_shape),\n",
        "      Conv2D(16, (3, 3), activation='relu', padding='same', strides=2), # downsample x2\n",
        "      Conv2D(8, (3, 3), activation='relu', padding='same', strides=2),  # downsample x2\n",
        "      Flatten(), # 8x8x8 = 512\n",
        "      Dense(latent_dim, activation='relu')\n",
        "    ])\n",
        "\n",
        "    latent_img_shape = (8,int(image_shape[0]/4),int(image_shape[1]/4))\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      Input(shape=(latent_dim,)),\n",
        "      Dense(np.prod(latent_img_shape)),\n",
        "      Reshape(latent_img_shape),\n",
        "      Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      Conv2D(image_shape[2], kernel_size=(3, 3), activation='sigmoid', padding='same') # back to color\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "latent_dim = 64 \n",
        "image_shape = x_train[0].shape\n",
        "autoencoder = ConvAutoencoderV2(latent_dim, image_shape)\n",
        "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "autoencoder.build(x_test.shape)\n",
        "autoencoder.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Approach 2: Implicit Neural Representations\n",
        "\n",
        "Implicit Neural Representations are a new way to represent images. This approach is very different from the classical CNNs that we have seen so far. \n",
        "The idea is to learn a function that maps a coordinate to a color value. These representations are very powerful for 3D scene representations, but can also be used for 2D images.\n",
        "\n",
        "For a starter code see our [Implicit Neural Representations with PyTorch Tutorial](https://colab.research.google.com/github/Digital-Media/vco/blob/main/11_IMLP.ipynb).\n",
        "\n",
        "Note that it is okay to only use a limited set of images.\n",
        "Explore the network size, the number of training epochs, and the positional encoding to get a feeling for the algorithm. Once you have a good understanding of the algorithm, you can try to compress images with it and compare it to the autoencoder approach (and to HW01)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOLKoIL6cuHNt5yw/4vZsbm",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "HW02_ML.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
