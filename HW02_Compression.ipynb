{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-intro-title"
      },
      "source": [
        "# Computer Vision Homework 02 : Image Compression with Frequency Domain Techniques and Deep Learning\n",
        "\n",
        "Contact: David C. Schedl (david.schedl@fh-hagenberg.at)\n",
        "\n",
        "Note: this is the starter pack for the **Visual Computing** homework. You do not need to use this template!\n",
        "\n",
        "This notebook combines concepts from previous exercises, focusing on image compression using both classical frequency-domain methods and modern deep learning approaches. We will be using the CelebA dataset for our experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-task-description"
      },
      "source": [
        "## Task Overview:\n",
        "The goal is to implement and compare different image compression algorithms.\n",
        "\n",
        "Choose at least two options from the following 3: \n",
        "1.  **Frequency Domain Compression:** Exploit the frequency domain (DFT/DCT) to compress images. Analyze how removing frequency coefficients affects image quality and compression ratio.\n",
        "2.  **Deep Learning Compression:** Implement and train autoencoders (both MLP-based and convolutional) to learn compressed representations of images. Investigate the trade-off between latent dimension size and reconstruction quality.\n",
        "3.  **Implicit Neural Representations:** Consider how INRs could be used for image representation and compression.\n",
        "\n",
        "Analyze your algorithms on the CelebA dataset. Evaluate results concerning quality (e.g., MSE) and size reduction. Consider how parameters can be tuned for different quality/size trade-offs.\n",
        "\n",
        "*Hint:* Work with resized images (e.g., 32x32, 64x64 or 128x128) and potentially smaller network architectures to speed up computation and manage storage requirements, especially during initial exploration.\n",
        "\n",
        "**Have fun!** ðŸ˜¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new-imports",
        "outputId": "eaf2a4b2-8384-41b6-b428-b8033e46fb42"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# use %% capture suppress any output for pip install\n",
        "%pip install tensorflow_datasets\n",
        "%pip install tensorflow\n",
        "%pip install bs4\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import cv2 # OpenCV for DFT/DCT\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow.keras.losses as losses\n",
        "from tensorflow.keras.layers import (Dense, Flatten, Conv2D, MaxPooling2D,\n",
        "                                     Conv2DTranspose, Reshape, Input)\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "def MSE(A,B):\n",
        "  \"\"\"compute the mean squared error (MSE) between numpy array A and B\n",
        "  \"\"\"\n",
        "  # Ensure A and B are float types for subtraction, and normalize if they are in 0-255 range\n",
        "  A = A.astype(np.float32)\n",
        "  B = B.astype(np.float32)\n",
        "  if A.max() > 1.0: A = A / 255.0\n",
        "  if B.max() > 1.0: B = B / 255.0\n",
        "  return ((A - B)**2).mean(axis=None)\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-celeba-intro"
      },
      "source": [
        "## Dataset: CelebA (CelebFaces Attributes Dataset)\n",
        "\n",
        "CelebA is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. We will use these images for our compression experiments. We'll resize them to a smaller dimension (e.g., 32x32 or 64x64) for manageability.\n",
        "\n",
        "We will use `tensorflow_datasets` to load and preprocess the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "new-celeba-load",
        "outputId": "8816412c-efa0-4474-ca46-3f0e6415d22b"
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT = 32\n",
        "IMG_WIDTH = 32\n",
        "IMG_CHANNELS = 3\n",
        "IMAGE_SHAPE = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
        "BATCH_SIZE = 64 # Adjusted for potentially larger dataset\n",
        "\n",
        "def preprocess_celeba(features):\n",
        "    image = features['image']\n",
        "    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n",
        "    # Crop to center to ensure 64x64 if resize introduces non-square aspect from original\n",
        "    # Or ensure original aspect ratio is maintained then crop, common practice for CelebA\n",
        "    # For simplicity, we'll assume resize gets us close enough for this exercise\n",
        "    # image = tf.image.central_crop(image, central_fraction=0.8) # Example if needed\n",
        "    # image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH]) # Resize again after crop\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, image # For autoencoders, input and target are the same\n",
        "\n",
        "# Load CelebA dataset\n",
        "ds_builder = tfds.builder('celeb_a')\n",
        "ds_builder.download_and_prepare()\n",
        "\n",
        "ds_train = ds_builder.as_dataset(split='train', shuffle_files=True)\n",
        "ds_train = ds_train.map(preprocess_celeba, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_train = ds_train.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "ds_test = ds_builder.as_dataset(split='test', shuffle_files=False) # No need to shuffle test\n",
        "ds_test = ds_test.map(preprocess_celeba, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "ds_test = ds_test.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f\"Training dataset: {ds_train}\")\n",
        "print(f\"Test dataset: {ds_test}\")\n",
        "\n",
        "# Display some sample images\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i, (image, _) in enumerate(ds_train.take(1).unbatch().take(5)):\n",
        "    ax = plt.subplot(1, 5, i + 1)\n",
        "    plt.imshow(image.numpy())\n",
        "    plt.title(f\"Sample {i+1}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"Sample Training Images from CelebA (Resized)\")\n",
        "plt.show()\n",
        "\n",
        "# Get a single image for DFT/DCT examples\n",
        "sample_celeba_image_rgb_normalized = next(iter(ds_train.take(1).unbatch()))[0].numpy()\n",
        "sample_celeba_image_rgb_0_255 = (sample_celeba_image_rgb_normalized * 255).astype(np.uint8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-freq-domain-title"
      },
      "source": [
        "# Option 1: Frequency Domain Compression\n",
        "\n",
        "In this part, we'll explore using the Discrete Fourier Transform (DFT) and Discrete Cosine Transform (DCT) to compress images. The core idea is that many images have significant information concentrated in lower frequencies. By transforming an image into its frequency components, we can discard or quantize less important (often high-frequency) components to achieve compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-dft-dct-explanation"
      },
      "source": [
        "### Frequencies: DFT and DCT\n",
        "\n",
        "The Discrete Fourier Transformation and the Discrete Cosine Transform convert a signal (like an image) into its constituent frequencies. Unlike the complex-valued output of a standard Fast Fourier Transformation (FFT) on real signals, OpenCV's DFT can be handled to work with magnitudes, and DCT directly produces real-numbered coefficients, which can be easier to work with for certain compression schemes.\n",
        "\n",
        "Below are examples using OpenCV's DFT and DCT implementations on a sample CelebA image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "new-dft-example",
        "outputId": "d8be7a72-24bd-45e9-8b44-4ab18cc9934e"
      },
      "outputs": [],
      "source": [
        "# Convert the sample CelebA image to grayscale for DFT/DCT\n",
        "gray_celeba = cv2.cvtColor(sample_celeba_image_rgb_0_255, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "# DFT Example\n",
        "# Transform the image to frequency domain using DFT\n",
        "# OpenCV's DFT output is a 2-channel array (real and imaginary parts)\n",
        "dft_input = gray_celeba.astype(np.float32) # DFT works best with float32\n",
        "dft_output = cv2.dft(dft_input, flags=cv2.DFT_COMPLEX_OUTPUT)\n",
        "\n",
        "# Shift the zero-frequency component to the center for visualization\n",
        "dft_shift = np.fft.fftshift(dft_output)\n",
        "\n",
        "# Calculate magnitude spectrum: log(1 + sqrt(real^2 + imag^2))\n",
        "magnitude_spectrum = 20 * np.log(cv2.magnitude(dft_shift[:,:,0], dft_shift[:,:,1]) + 1)\n",
        "\n",
        "# Inverse DFT\n",
        "# First, shift back\n",
        "f_ishift = np.fft.ifftshift(dft_shift)\n",
        "# Then, inverse DFT\n",
        "img_back_dft = cv2.idft(f_ishift, flags=cv2.DFT_SCALE | cv2.DFT_REAL_OUTPUT) # Scale and get real output\n",
        "img_back_dft = np.clip(img_back_dft, 0, 255).astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.subplot(131), plt.imshow(gray_celeba, cmap='gray'), plt.title('Input Grayscale Image')\n",
        "plt.subplot(132), plt.imshow(magnitude_spectrum, cmap='gray'), plt.title('DFT Magnitude Spectrum')\n",
        "plt.subplot(133), plt.imshow(img_back_dft, cmap='gray'), plt.title('Image after IDFT')\n",
        "plt.show()\n",
        "\n",
        "print(f'MSE between original and DFT reconstructed: {MSE(gray_celeba, img_back_dft)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "new-dct-example",
        "outputId": "ffab6f98-a638-4c5d-a167-0deb1b787894"
      },
      "outputs": [],
      "source": [
        "# DCT Example\n",
        "dct_input = gray_celeba.astype(np.float32) # DCT also needs float input\n",
        "\n",
        "# Transform the image to frequency domain using DCT\n",
        "dct_output = cv2.dct(dct_input)\n",
        "log_dct_spectrum = np.log(np.abs(dct_output) + 1) # Log scale for visualization\n",
        "\n",
        "# Inverse DCT\n",
        "img_back_dct = cv2.idct(dct_output)\n",
        "img_back_dct = np.clip(img_back_dct, 0, 255).astype(np.uint8)\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.subplot(131), plt.imshow(gray_celeba, cmap='gray'), plt.title('Input Grayscale Image')\n",
        "plt.subplot(132), plt.imshow(log_dct_spectrum, cmap='gray'), plt.title('DCT Spectrum (Log Scaled)')\n",
        "plt.subplot(133), plt.imshow(img_back_dct, cmap='gray'), plt.title('Image after IDCT')\n",
        "plt.show()\n",
        "\n",
        "print(f'MSE between original and DCT reconstructed: {MSE(gray_celeba, img_back_dct)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-freq-domain-task"
      },
      "source": [
        "### Your Task (Frequency Domain Compression):\n",
        "1.  **Implement Compression:** Modify the DFT or DCT process. After transforming the image to the frequency domain, set a certain percentage of the smallest (in magnitude) coefficients to zero, or all coefficients outside a certain low-frequency region (e.g., top-left corner for DCT, center for shifted DFT).\n",
        "2.  **Analyze:** How does varying the number of removed/zeroed coefficients affect the reconstructed image quality (visual and MSE)?\n",
        "3.  **Evaluate:** Estimate the compression ratio. For example, if you keep only K out of N coefficients, how does this translate to potential bit savings (e.g., by only storing non-zero coefficients and their positions, or using run-length encoding on zeroed coefficients)?\n",
        "4.  **Parameterize:** Can you make the image quality / size reduction a parameter of your compression algorithm (e.g., a quality factor or a percentage of coefficients to keep)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-autoencoder-intro"
      },
      "source": [
        "# Option 2: Autoencoders\n",
        "\n",
        "An autoencoder is a neural network that learns efficient codings of unlabeled data. It consists of two parts: an **encoder** that maps the input to a lower-dimensional latent representation, and a **decoder** that reconstructs the input from this latent representation. If the latent representation is significantly smaller than the input data, the autoencoder effectively performs compression.\n",
        "\n",
        "We will implement and train autoencoders on the CelebA dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-mlp-ae-title"
      },
      "source": [
        "### Example AutoEncoder with Multi-Layer Perceptrons (Dense layers)\n",
        "\n",
        "This is a simple autoencoder using dense layers. The encoder flattens the image and maps it to a latent vector, and the decoder maps this vector back to the image shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "new-mlp-ae-code",
        "outputId": "cd18cb6f-ab36-4f32-f8d0-0e1a362ff062"
      },
      "outputs": [],
      "source": [
        "class AutoencoderMLP(Model):\n",
        "  def __init__(self, latent_dim, image_shape):\n",
        "    super(AutoencoderMLP, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.image_shape = image_shape\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      Flatten(),\n",
        "      Dense(latent_dim * 4, activation='relu'), # Intermediate layer\n",
        "      Dense(latent_dim, activation='relu'),\n",
        "    ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      Dense(latent_dim * 4, activation='relu'), # Intermediate layer\n",
        "      Dense(np.prod(self.image_shape), activation='sigmoid'), # Sigmoid for [0,1] output\n",
        "      Reshape(self.image_shape)\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "latent_dim_mlp = 32\n",
        "autoencoder_mlp = AutoencoderMLP(latent_dim_mlp, IMAGE_SHAPE)\n",
        "autoencoder_mlp.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "# Build the model by passing a sample batch shape\n",
        "autoencoder_mlp(tf.ones((1,) + IMAGE_SHAPE))\n",
        "autoencoder_mlp.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-mlp-ae-training-title"
      },
      "source": [
        "### Training the MLP Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new-mlp-ae-training-code",
        "outputId": "54f3d745-6c66-480c-d992-f31c9ba3cf3d"
      },
      "outputs": [],
      "source": [
        "epochs_mlp = 10 # Adjust as needed, CelebA might need more epochs\n",
        "print(f\"Training MLP Autoencoder for {epochs_mlp} epochs...\")\n",
        "history_mlp = autoencoder_mlp.fit(ds_train,\n",
        "                                epochs=epochs_mlp,\n",
        "                                shuffle=True, # Already shuffled by ds_train, but good practice\n",
        "                                validation_data=ds_test,\n",
        "                                callbacks=[tensorboard_callback],\n",
        "                                verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-mlp-ae-eval-title"
      },
      "source": [
        "### Encoding / Decoding with MLP Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "new-mlp-ae-eval-code",
        "outputId": "18e9dbf7-a744-4275-b45a-d0702750b13e"
      },
      "outputs": [],
      "source": [
        "print(\"Encoding and decoding test images with MLP Autoencoder...\")\n",
        "# Get a batch of test images for display\n",
        "test_images_batch, _ = next(iter(ds_test))\n",
        "\n",
        "encoded_imgs_mlp = autoencoder_mlp.encoder(test_images_batch).numpy()\n",
        "decoded_imgs_mlp = autoencoder_mlp.decoder(encoded_imgs_mlp).numpy()\n",
        "\n",
        "print(f\"Shape of encoded images (MLP): {encoded_imgs_mlp.shape}\")\n",
        "\n",
        "n = 5 # Number of images to display\n",
        "plt.figure(figsize=(20, 8))\n",
        "for i in range(n):\n",
        "  # Display original\n",
        "  ax = plt.subplot(2, n, i + 1)\n",
        "  plt.imshow(test_images_batch[i].numpy())\n",
        "  plt.title(\"original\")\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # Calculate MSE for this image\n",
        "  mse_val = MSE(test_images_batch[i].numpy(), decoded_imgs_mlp[i])\n",
        "\n",
        "  # Display reconstruction\n",
        "  ax = plt.subplot(2, n, i + 1 + n)\n",
        "  plt.imshow(decoded_imgs_mlp[i])\n",
        "  plt.title(f\"MSE:{mse_val:.2E}\")\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.suptitle(\"MLP Autoencoder: Original vs. Reconstructed\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-conv-ae-title"
      },
      "source": [
        "### Example AutoEncoders with Convolutional Layers\n",
        "\n",
        "Convolutional Autoencoders (CAEs) typically perform better for image data as they preserve spatial structures. The encoder uses convolutional layers to downsample the image, and the decoder uses transposed convolutional layers to upsample it back.\n",
        "\n",
        "**Note on `IMAGE_SHAPE`**: Ensure `IMAGE_SHAPE` (defined during CelebA loading as `(64, 64, 3)`) is used by these models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "new-conv-ae-v1-code",
        "outputId": "93493026-9761-404d-ab6f-0a4c003ecfb0"
      },
      "outputs": [],
      "source": [
        "# Convolutional Autoencoder V1 (No dense latent vector, latent space is a feature map)\n",
        "class ConvAutoencoderV1(Model):\n",
        "  def __init__(self, image_shape):\n",
        "    super(ConvAutoencoderV1, self).__init__()\n",
        "    self.image_shape = image_shape\n",
        "    # Encoder: (64,64,3) -> (32,32,16) -> (16,16,32) -> (8,8,64)\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      Input(shape=self.image_shape),\n",
        "      Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
        "      Conv2D(32, (3, 3), activation='relu', padding='same', strides=2),\n",
        "      Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
        "    ])\n",
        "\n",
        "    # Decoder: (8,8,64) -> (16,16,32) -> (32,32,16) -> (64,64,3)\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      Conv2DTranspose(self.image_shape[2], kernel_size=3, strides=2, activation='sigmoid', padding='same') # Output channels = 3\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "autoencoder_conv_v1 = ConvAutoencoderV1(IMAGE_SHAPE)\n",
        "autoencoder_conv_v1.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "autoencoder_conv_v1(tf.ones((BATCH_SIZE,) + IMAGE_SHAPE))\n",
        "autoencoder_conv_v1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "new-conv-ae-v2-code",
        "outputId": "12b527b5-6ff9-4043-f03f-aeab87a24aab"
      },
      "outputs": [],
      "source": [
        "# Convolutional Autoencoder V2 (with a dense latent vector)\n",
        "class ConvAutoencoderV2(Model):\n",
        "  def __init__(self, latent_dim, image_shape):\n",
        "    super(ConvAutoencoderV2, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.image_shape = image_shape\n",
        "\n",
        "    # Calculate shape before Flatten in encoder\n",
        "    # Input: (64,64,3)\n",
        "    # Conv1 (strides 2): (32,32,16)\n",
        "    # Conv2 (strides 2): (16,16,32)\n",
        "    # Conv3 (strides 2): (8,8,64) <- This is self.shape_before_flatten\n",
        "    self.shape_before_flatten = (image_shape[0]//8, image_shape[1]//8, 64)\n",
        "\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      Input(shape=self.image_shape),\n",
        "      Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
        "      Conv2D(32, (3, 3), activation='relu', padding='same', strides=2),\n",
        "      Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
        "      Flatten(),\n",
        "      Dense(self.latent_dim, activation='relu')\n",
        "    ])\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      Input(shape=(self.latent_dim,)),\n",
        "      Dense(np.prod(self.shape_before_flatten), activation='relu'),\n",
        "      Reshape(self.shape_before_flatten),\n",
        "      Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
        "      Conv2DTranspose(self.image_shape[2], kernel_size=3, strides=2, activation='sigmoid', padding='same')\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "latent_dim_conv = 128 # Example latent dimension for ConvAE V2\n",
        "autoencoder_conv_v2 = ConvAutoencoderV2(latent_dim_conv, IMAGE_SHAPE)\n",
        "autoencoder_conv_v2.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "autoencoder_conv_v2(tf.ones((BATCH_SIZE,) + IMAGE_SHAPE))\n",
        "autoencoder_conv_v2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-conv-ae-training-task"
      },
      "source": [
        "### Your Task (Autoencoders):\n",
        "1.  **Train the Autoencoders:** Train `ConvAutoencoderV1` and `ConvAutoencoderV2` (and optionally the MLP one further) on the CelebA dataset. Monitor training using TensorBoard.\n",
        "2.  **Experiment with `latent_dim`:** For `AutoencoderMLP` and `ConvAutoencoderV2`, vary the `latent_dim`. How does this affect reconstruction quality and the potential compression ratio?\n",
        "3.  **Analyze Network Architecture:** How do choices in the number of layers, filter sizes, or activation functions impact performance?\n",
        "4.  **Evaluate:** Compare the reconstructed images with the originals (visually and using MSE). How does the compression achieved by autoencoders compare to the frequency-domain methods and to standard image formats (e.g., JPEG at different quality levels, if you want to go further)?\n",
        "5.  **Size Reduction:** The size of the latent representation (`latent_dim` for MLP/ConvV2, or the dimensions of the feature map for ConvV1) directly relates to the compressed size. Discuss how you would store these latent vectors/tensors and how that compares to the original image size in bits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-inr-intro"
      },
      "source": [
        "# Option 3: Implicit Neural Representations (INRs)\n",
        "\n",
        "Implicit Neural Representations (INRs), sometimes called coordinate-based neural networks, offer a different paradigm for representing signals like images. Instead of learning a discrete grid of pixels, an INR learns a continuous function `f(coordinate) -> value` (e.g., `f(x,y) -> (R,G,B)`).\n",
        "\n",
        "The network itself (its weights) becomes the compressed representation of the image. To reconstruct the image, you query the network with all pixel coordinates.\n",
        "\n",
        "**Key characteristics:**\n",
        "*   **Continuous Representation:** Can be sampled at arbitrary resolutions.\n",
        "*   **Compression:** The size of the network parameters determines the compressed size.\n",
        "*   **Positional Encoding:** Often crucial for INRs to learn high-frequency details.\n",
        "\n",
        "For a starter code and more detailed explanation, refer to tutorials on INRs/IMLPs, such as:\n",
        "*   [Implicit Neural Representations with PyTorch Tutorial (NeRF-related)](https://colab.research.google.com/github/Digital-Media/vco/blob/main/11_IMLP.ipynb) \n",
        "\n",
        "### Your Task (Implicit Neural Representations - Exploration):\n",
        "1.  **Understand the Concept:** Review the principles of INRs for image representation.\n",
        "2.  **Experiment (Optional but Encouraged):** If time permits, try to fit an INR (e.g., a simple MLP with positional encoding) to a single CelebA image.\n",
        "    *   How does network size (layers, neurons) affect the quality of the fit and the compression (model size)?\n",
        "    *   How many training epochs are needed?\n",
        "    *   How does positional encoding influence the result?\n",
        "3.  **Compare:** Conceptually, how does compression with INRs compare to autoencoders and frequency-domain methods in terms of:\n",
        "    *   How the compressed data is stored?\n",
        "    *   Decoding speed?\n",
        "    *   Ability to represent details?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "new-final-remarks"
      },
      "source": [
        "# Final Report and Comparison\n",
        "\n",
        "Summarize your findings from all parts of this exercise in a report.\n",
        "*   Compare the different compression techniques (DFT/DCT, MLP Autoencoder, Convolutional Autoencoders, and conceptually INRs).\n",
        "*   Discuss their strengths and weaknesses regarding compression ratio, reconstruction quality, computational cost (training/inference), and parameter tuning.\n",
        "*   What are the trade-offs involved in choosing a particular method or its parameters?\n",
        "*   How do these methods compare to your results from previous homeworks (if applicable, e.g., HW01's frequency compression on CIFAR-10 vs. CelebA)?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
