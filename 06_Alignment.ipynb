{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU831zdvOJ86"
      },
      "source": [
        "# Tutorial 06 - Alignment\n",
        "\n",
        "## Dr. David C. Schedl\n",
        "\n",
        "Note: this tutorial is geared towards students **experienced in programming** and aims to introduce you to **Computer Vision** techniques.\n",
        "\n",
        "\n",
        "Useful links:\n",
        "* OpenCV Tutorials: https://docs.opencv.org/master/d9/df8/tutorial_root.html\n",
        "* Image Processing in Pyhton: https://github.com/xn2333/OpenCV/blob/master/Seminar_Image_Processing_in_Python.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFUPuyJVOJ88"
      },
      "source": [
        "# Table of Contents  \n",
        "\n",
        "- Initialization\n",
        "- SIFT\n",
        "- Least Squares Model\n",
        "- Missmatches\n",
        "- Warping\n",
        "- Everything Combined\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkE2YAvyOJ88"
      },
      "source": [
        "<a id=\"Initialization\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weMMc2WZlsJc"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkUzZ6lWK_S5"
      },
      "source": [
        "As always let's import useful libraries, first.\n",
        "We will work with images today. So let's download some with `curl`.\n",
        "Let's define utility functions to display images, in Jupyter Notebooks. OpenCV's `imshow` does not work and matplotlib's `imshow` needs special treatment due to color channel handling (RGB vs. BGR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtUm6wJFOJ89"
      },
      "outputs": [],
      "source": [
        "# import the libraries we use\n",
        "import os\n",
        "import cv2 # openCV\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# loading images\n",
        "!curl -o \"campus.jpg\" \"https://raw.githubusercontent.com/Digital-Media/cv_data/main/panorama_stitching/campus_hagenberg.jpg\"\n",
        "!curl -o \"foto1A.jpg\" \"https://raw.githubusercontent.com/Digital-Media/cv_data/main/panorama_stitching/UTA_foto1A.jpg\"\n",
        "!curl -o \"foto1B.jpg\" \"https://raw.githubusercontent.com/Digital-Media/cv_data/main/panorama_stitching/UTA_foto1B.jpg\"\n",
        "\n",
        "# utility function(s)\n",
        "def imshow(image, *args, **kwargs):\n",
        "    \"\"\"A replacement for cv2.imshow() for use in Jupyter notebooks using matplotlib.\n",
        "\n",
        "        Args:\n",
        "          image : np.ndarray. shape (N, M) or (N, M, 1) is an NxM grayscale image. shape\n",
        "            (N, M, 3) is an NxM BGR color image.\n",
        "    \"\"\"\n",
        "    if len(image.shape) == 3:\n",
        "      # Height, width, channels\n",
        "      # Assume BGR, do a conversion\n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Draw the image\n",
        "    plt.imshow(image, *args, **kwargs)\n",
        "    # We'll also disable drawing the axes and tick marks in the plot, since it's actually an image\n",
        "    plt.axis('off')\n",
        "    # Make sure it outputs\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHUusFMtOJ9A"
      },
      "source": [
        "# SIFT Descriptor and Feature Matching (from last Tutorial)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7ekT4WpigMC"
      },
      "source": [
        "## Implementation of the Nearest Neighbor Distance Ratio Algorithm\n",
        "\n",
        "This is the implementation of the previous tutorial.\n",
        "Make sure that you have a recent OpenCV version installed. SIFT was patented and is only free since 2020.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mb0_FCAIE9gO"
      },
      "outputs": [],
      "source": [
        "def get_matched_points(kpsA, kpsB, featuresA, featuresB, matches):\n",
        "    # convert the keypoints to numpy arrays\n",
        "    kpsA = np.float32([kp.pt for kp in kpsA])\n",
        "    kpsB = np.float32([kp.pt for kp in kpsB])\n",
        "\n",
        "    if len(matches) > 4:\n",
        "\n",
        "        # construct the two sets of points\n",
        "        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])\n",
        "        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])\n",
        "\n",
        "        return (ptsA, ptsB)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def get_nn_distance_ratio_matches(featuresA, featuresB, ratio=0.5):\n",
        "    # Nearest Neigbhor Distance Ratio matching\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False) # init OpenCV's matcher\n",
        "\n",
        "    rawMatches = bf.knnMatch(featuresB, featuresA, 2) # retrieves the two nearest neighbours\n",
        "    matches = []\n",
        "\n",
        "    # loop over the raw matches\n",
        "    for m,n in rawMatches:\n",
        "        # ensure the distance is within a certain ratio of each\n",
        "        # other (i.e. Lowe's ratio test)\n",
        "        if m.distance < n.distance * ratio:\n",
        "            matches.append(m)\n",
        "\n",
        "    # sort based on the distance of the closest neighbour\n",
        "    matches = sorted(matches, key = lambda x:x.distance)\n",
        "\n",
        "    return matches\n",
        "\n",
        "\n",
        "def do_sift_matching( imgA, imgB, ratio=0.5, show=True ):\n",
        "\n",
        "\n",
        "    # convert to grayscale\n",
        "    imgA_gray = cv2.cvtColor(imgA, cv2.COLOR_RGB2GRAY)\n",
        "    imgB_gray = cv2.cvtColor(imgB, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # init sift descriptor\n",
        "    sift = cv2.SIFT_create()\n",
        "\n",
        "    # get keypoints and descriptors\n",
        "    (kpsA, featuresA) = sift.detectAndCompute(imgA_gray, None)\n",
        "    (kpsB, featuresB) = sift.detectAndCompute(imgB_gray, None) # compute features for second image\n",
        "\n",
        "    # do nn matching\n",
        "    matches = get_nn_distance_ratio_matches(featuresA, featuresB, ratio)\n",
        "\n",
        "    (ptsA, ptsB) = get_matched_points(kpsB, kpsA, featuresB, featuresA, matches[:])\n",
        "\n",
        "    if show:\n",
        "        # optionally, display (some) matches\n",
        "        img_matches = cv2.drawMatches(imgB_gray,kpsB,imgA_gray,kpsA,matches[:1000:10],\n",
        "                                None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "        plt.figure(figsize=(20,10)) # this command makes the figure larger so we see the images better\n",
        "        imshow(img_matches)\n",
        "        plt.show()\n",
        "\n",
        "    return ptsA, ptsB, matches\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ6KH-Nx25kq"
      },
      "source": [
        "## Prepare the 'Campus' images\n",
        "\n",
        "Create two images by extracting overlapping subregions from a large image.\n",
        "These can be stitched by finding correspondences.\n",
        "\n",
        "The ground truth translation is [449, -41]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Qm_NcD0E9gC"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "# read images and transform them to grayscale\n",
        "# Make sure that the train image is the image that will be transformed\n",
        "\n",
        "huge_img = cv2.imread(\"campus.jpg\")\n",
        "print(huge_img.shape)\n",
        "w = 1250\n",
        "h = math.floor(w*huge_img.shape[0]/huge_img.shape[1])\n",
        "print(h)\n",
        "\n",
        "# create image A and B by extracting overlapping subregions from a large image\n",
        "img = cv2.resize( huge_img, (w,h) )\n",
        "# write the image to disk\n",
        "cv2.imwrite(\"campus_1250px.jpg\", img)\n",
        "print(img.shape)\n",
        "imgA = img[:400,    -801:-1,:]\n",
        "imgB = img[-401:-1, :800,   :]\n",
        "\n",
        "# display\n",
        "plt.figure(figsize=(25,10)) # this command makes the figure larger so we see the images better\n",
        "plt.subplot(121), imshow(imgA, cmap=\"gray\"), plt.title('image A')\n",
        "plt.subplot(122), imshow(imgB, cmap=\"gray\"), plt.title('image B')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugpnHM-Q25kr"
      },
      "source": [
        "## Feature Matching on the 'Campus' images\n",
        "\n",
        "Let's do feature matching on the 'Campus' images with a nearest-neighbor distance ratio of `.5`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX1Gof1G25ks"
      },
      "outputs": [],
      "source": [
        "(ptsA, ptsB, matches) = do_sift_matching( imgA, imgB, ratio=0.5 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRRWHPy7zZBe"
      },
      "source": [
        "**Geometric interpretation**: Let's look at the offsets (Î”x and Î”y) of matched features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0cMy9f7oYUV"
      },
      "outputs": [],
      "source": [
        "# plot the Î”x and Î”y of matched points\n",
        "diff = ptsA - ptsB # compute offsets between matched points\n",
        "simDistance = np.float32([m.distance for m in matches])\n",
        "\n",
        "# display\n",
        "fig = plt.figure(figsize=(15,10)) # this command makes the figure larger so we see the images better\n",
        "ax = fig.add_subplot(111)\n",
        "plt.scatter(diff[:,0],diff[:,1],c=simDistance,cmap='viridis')\n",
        "#plt.xlim((0,500)), plt.ylim((-100,0))\n",
        "\n",
        "ax.set_aspect('equal', adjustable='box'), plt.xlabel('$\\Delta x$'), plt.ylabel('$\\Delta y$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQZNSl2r25ks"
      },
      "source": [
        "**Another Geometric interpretation**: Let's look at the mappings from $x$ to $x'$ for each axis (horizontal and vertical) seperately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1T41ucL0N2u"
      },
      "outputs": [],
      "source": [
        "\n",
        "# display\n",
        "fig = plt.figure(figsize=(20,10)) # this command makes the figure larger so we see the images better\n",
        "plt.subplot(121), plt.scatter(ptsA[:,0], ptsB[:,0]), plt.title( 'x coordinate' )\n",
        "plt.xlabel('${\\\\bf x}_x$ (image A)'), plt.ylabel(\"${\\\\bf x'}_x$ (image B)\")\n",
        "plt.xlim((0,np.max((ptsA[:,0])))), plt.ylim((0,np.max((ptsB[:,0]))))\n",
        "plt.subplot(122), plt.scatter(ptsA[:,1], ptsB[:,1]), plt.title( 'y coordinate' )\n",
        "plt.xlabel('${\\\\bf x}_y$ (image A)'), plt.ylabel(\"${\\\\bf x'}_y$ (image B)\")\n",
        "plt.xlim((0,np.max((ptsA[:,1])))), plt.ylim((0,np.max((ptsB[:,1]))))\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A7e02WK25kt"
      },
      "source": [
        "# Least Squares Model\n",
        "\n",
        "Let's use a least-squares solver to find the parameters $t$ by minimizing the sum of squared residuals.\n",
        "The model is $x' = x - t$.\n",
        "\n",
        "Remember the ground truth translation ($t$) is [449, -41]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iL2Q_Zh10N2v"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import least_squares\n",
        "\n",
        "# define the translational model\n",
        "def transl_model(t):\n",
        "    residual = ptsA - (ptsB + t)\n",
        "    return np.linalg.norm( residual, axis=1 )\n",
        "\n",
        "\n",
        "t0 = np.array([0, 0]) # initial guess\n",
        "res = least_squares(transl_model, t0) # solve with least squares solver\n",
        "print(res.x)\n",
        "\n",
        "# compute the x' for plotting\n",
        "modelB = ptsA - res.x\n",
        "\n",
        "# display\n",
        "fig = plt.figure(figsize=(20,10)) # this command makes the figure larger so we see the images better\n",
        "plt.subplot(121), plt.scatter(ptsA[:,0], ptsB[:,0]), plt.title( 'x coordinate' )\n",
        "plt.plot(ptsA[:,0], modelB[:,0], color='red')\n",
        "plt.xlabel('${\\\\bf x}_x$ (image A)'), plt.ylabel(\"${\\\\bf x'}_x$ (image B)\")\n",
        "plt.xlim((0,np.max((ptsA[:,0])))), plt.ylim((0,np.max((ptsB[:,0]))))\n",
        "plt.subplot(122), plt.scatter(ptsA[:,1], ptsB[:,1]), plt.title( 'y coordinate' )\n",
        "plt.plot(ptsA[:,1], modelB[:,1], color='red')\n",
        "plt.xlabel('${\\\\bf x}_y$ (image A)'), plt.ylabel(\"${\\\\bf x'}_y$ (image B)\")\n",
        "plt.xlim((0,np.max((ptsA[:,1])))), plt.ylim((0,np.max((ptsB[:,1]))))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRznUyMulZyQ"
      },
      "source": [
        "The estimated parameters are very close to the ground truth.\n",
        "Remember the ground truth translation ($t$) is [449, -41]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYTSV4Cf25kt"
      },
      "source": [
        "# Missmatches\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxT_UtCi25kt"
      },
      "source": [
        "## Erroneous Feature Matching on the 'Campus' images\n",
        "\n",
        "Sometimes missmatches are introduced by feature matching.\n",
        "Let's simulate this with a nearest-neighbor distance ratio of `.95`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmoISILo25ku"
      },
      "outputs": [],
      "source": [
        "(ptsA, ptsB, matches) = do_sift_matching( imgA, imgB, ratio=0.95 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88iHMlaV25ku"
      },
      "source": [
        "## Least Squares\n",
        "\n",
        "Outliers are a huge problem for LS!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4crjsCB25ku"
      },
      "outputs": [],
      "source": [
        "from scipy.optimize import least_squares\n",
        "\n",
        "# define the translational model\n",
        "def transl_model(t):\n",
        "    residual = ptsA - (ptsB + t)\n",
        "    return np.linalg.norm( residual, axis=1 )\n",
        "\n",
        "\n",
        "t0 = np.array([0, 0]) # initial guess\n",
        "res = least_squares(transl_model, t0)\n",
        "print(res.x)\n",
        "\n",
        "# compute the x' for plotting\n",
        "modelB = ptsA - res.x\n",
        "\n",
        "# display\n",
        "fig = plt.figure(figsize=(20,10)) # this command makes the figure larger so we see the images better\n",
        "plt.subplot(121), plt.scatter(ptsA[:,0], ptsB[:,0]), plt.title( 'x coordinate' )\n",
        "plt.plot(ptsA[:,0], modelB[:,0], color='red')\n",
        "plt.xlabel('${\\\\bf x}_x$ (image A)'), plt.ylabel(\"${\\\\bf x'}_x$ (image B)\")\n",
        "plt.xlim((0,np.max((ptsA[:,0])))), plt.ylim((0,np.max((ptsB[:,0]))))\n",
        "plt.subplot(122), plt.scatter(ptsA[:,1], ptsB[:,1]), plt.title( 'y coordinate' )\n",
        "plt.plot(ptsA[:,1], modelB[:,1], color='red')\n",
        "plt.xlabel('${\\\\bf x}_y$ (image A)'), plt.ylabel(\"${\\\\bf x'}_y$ (image B)\")\n",
        "plt.xlim((0,np.max((ptsA[:,1])))), plt.ylim((0,np.max((ptsB[:,1]))))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZOHjj_BlmkE"
      },
      "source": [
        "Again the ground truth translation ($t$) is [449, -41], but the least-squares solution is pretty off this time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1S42CBL25ku"
      },
      "source": [
        "## RANSAC (RANdom SAmple Consensus)\n",
        "\n",
        "RANSAC on the other hand, can deal with outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XyXsPvr0N2v"
      },
      "outputs": [],
      "source": [
        "# estimate using RANSAC\n",
        "(H, status) = cv2.estimateAffine2D(ptsB, ptsA, cv2.RANSAC)\n",
        "print(H[:,2]) # translation\n",
        "\n",
        "modelB = ptsA - H[:,2]\n",
        "\n",
        "# display\n",
        "fig = plt.figure(figsize=(20,10)) # this command makes the figure larger so we see the images better\n",
        "plt.subplot(121), plt.scatter(ptsA[:,0], ptsB[:,0]), plt.title( 'x coordinate' )\n",
        "plt.plot(ptsA[:,0], modelB[:,0], color='red')\n",
        "plt.xlabel('${\\\\bf x}_x$ (image A)'), plt.ylabel(\"${\\\\bf x'}_x$ (image B)\")\n",
        "plt.xlim((0,np.max((ptsA[:,0])))), plt.ylim((0,np.max((ptsB[:,0]))))\n",
        "plt.subplot(122), plt.scatter(ptsA[:,1], ptsB[:,1]), plt.title( 'y coordinate' )\n",
        "plt.plot(ptsA[:,1], modelB[:,1], color='red')\n",
        "plt.xlabel('${\\\\bf x}_y$ (image A)'), plt.ylabel(\"${\\\\bf x'}_y$ (image B)\")\n",
        "plt.xlim((0,np.max((ptsA[:,1])))), plt.ylim((0,np.max((ptsB[:,1]))))\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unvCSi28zgZv"
      },
      "source": [
        "# Warping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxm7YrB6zj4_"
      },
      "source": [
        "## Translational Model\n",
        "\n",
        "Let's merge the two images by translating/warping one image (image A).\n",
        "So we assume that a point $\\bf x$ in one image corresponds to $\\bf x' = \\bf x + \\bf t$ in the second image, where $\\bf t=[449,-41]$ is the translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgBHhSxsE9go"
      },
      "outputs": [],
      "source": [
        "# the translation as matrix\n",
        "H = np.float32([\n",
        "\t[1, 0, 449],\n",
        "\t[0, 1, -41]\n",
        "])\n",
        "\n",
        "# make sure the new image is large enough\n",
        "width  = imgA.shape[1] + int(imgB.shape[1]/2)\n",
        "height = imgA.shape[0] + int(imgB.shape[0]/3)\n",
        "\n",
        "# warp image A to image B\n",
        "result = cv2.warpAffine(imgA, H, (width, height))\n",
        "result[0:imgB.shape[0], 0:imgB.shape[1]] = imgB # copy image B on top of warped image A\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "imshow(result)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX4ZWfKL3x95"
      },
      "source": [
        "## âŒ¨ï¸ Try it yourself: Implement your own transformation\n",
        "\n",
        "Play around with the transformation matrix and design your own warped image.\n",
        "\n",
        "Challenge:\n",
        " - Rotate the image by 90Â°.\n",
        " - Flip the image horizontally.\n",
        " - Can you combine multiple transformations? How? Does the order matter?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jjlr7Fkv4kLI"
      },
      "outputs": [],
      "source": [
        "# the transformation matrices:\n",
        "\n",
        "## ToDo: change matrix\n",
        "H = np.float32([\n",
        "\t[1, 0, 0],\n",
        "\t[0, 1, 0]\n",
        "])\n",
        "\n",
        "# make sure the new image is large enough\n",
        "width  = imgA.shape[1]*2\n",
        "height = imgA.shape[0]*2\n",
        "\n",
        "# warp image A to image B\n",
        "warped = cv2.warpAffine(imgA, H, (width, height))\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "imshow(warped)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N79omsa30GPa"
      },
      "source": [
        "# Everything (in one line): OpenCV's Image Stitcher\n",
        "\n",
        "We could have avoided computing features, matching them, estimating translation and warping by simply using OpenCV's image stitcher. It does something similar like what we did and furthermore smoothly blends images. But where is the fun in that? ðŸ˜ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YvbfVdJpcqo"
      },
      "outputs": [],
      "source": [
        "# initialize OpenCV's image stitcher object and then perform the image\n",
        "# stitching\n",
        "images = [imgA, imgB]\n",
        "stitcher = cv2.Stitcher_create( mode=1 ) # mode=0 is panorama and assumes spherical stitichg, mode=1 is scan\n",
        "(status, stitched) = stitcher.stitch(images)\n",
        "\n",
        "if status==0: # stitching worked!\n",
        "  plt.figure(figsize=(20,10))\n",
        "  imshow(stitched)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "06_Alignment.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "3e2b6404eb79c0fba1a251907aa00d26891e411eaa7ffd3f13ecfdd311f8c56c"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit (windows store)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
